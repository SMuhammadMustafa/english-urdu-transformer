{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bGG6ONlB-mdn",
    "outputId": "8f340fdd-cac5-46c4-d361-29232d1ff2a3"
   },
   "outputs": [],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lX6QMMN_Kc8",
    "outputId": "92ef7fdd-5032-4795-bf25-52b3a1a6e49b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CARA8Keu_bLa",
    "outputId": "0a2b438a-c57a-41b8-ee4e-15e2881dd9d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bible  quran\n"
     ]
    }
   ],
   "source": [
    "!ls \"drive/MyDrive/Colab Datasets/umc005-corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7g8iw0Oe9dRJ",
    "outputId": "872dcd2a-737a-4631-c9ec-410eb7a83f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.17.1\n",
      "GPU Available:  True\n",
      "GPU Devices:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Fri Nov 15 23:10:39 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   48C    P8              10W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU is detected\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"GPU Available: \", tf.test.is_built_with_cuda())\n",
    "print(\"GPU Devices: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Print GPU specifications\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1DMeXtCA9eBg",
    "outputId": "0156833e-32d3-4825-9b40-328cc576678a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Configure GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gxPhXVWFD06c"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZIutIlO-QpI",
    "outputId": "3a577bd2-a857-4bc2-be9a-381e6c58ed3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6414 English sentences and 6414 Urdu sentences\n",
      "\n",
      "First 3 sentence pairs:\n",
      "\n",
      "English: ﻿All praise be to Allah alone , the Sustainer of all the worlds .\n",
      "Urdu: ﻿سب تعریفیں اللہ ہی کے لئے ہیں جو تمام جہانوں کی پرورش فرمانے والا ہے ۔\n",
      "\n",
      "English: Most Compassionate , Ever - Merciful .\n",
      "Urdu: نہایت مہربان بہت رحم فرمانے والا ہے ۔\n",
      "\n",
      "English: Master of the Day of Judgment .\n",
      "Urdu: روزِ جزا کا مالک ہے ۔\n",
      "\n",
      "Epoch 1/15\n",
      "\n",
      "Processing chunk 1\n",
      "Chunk 1 Batch 0 Loss 9.0424\n",
      "Chunk 1 Batch 50 Loss 8.7492\n",
      "Memory usage: 1992.49 MB\n",
      "\n",
      "Processing chunk 2\n",
      "Chunk 2 Batch 0 Loss 8.6734\n",
      "Chunk 2 Batch 50 Loss 8.2863\n",
      "Memory usage: 1993.93 MB\n",
      "\n",
      "Processing chunk 3\n",
      "Chunk 3 Batch 0 Loss 8.2182\n",
      "Chunk 3 Batch 50 Loss 7.7078\n",
      "Memory usage: 1994.09 MB\n",
      "\n",
      "Processing chunk 4\n",
      "Chunk 4 Batch 0 Loss 7.5845\n",
      "Chunk 4 Batch 50 Loss 7.0233\n",
      "Memory usage: 1994.30 MB\n",
      "\n",
      "Processing chunk 5\n",
      "Chunk 5 Batch 0 Loss 6.8536\n",
      "Chunk 5 Batch 50 Loss 6.2576\n",
      "Memory usage: 1994.59 MB\n",
      "\n",
      "Processing chunk 6\n",
      "Chunk 6 Batch 0 Loss 6.3113\n",
      "Chunk 6 Batch 50 Loss 5.9617\n",
      "Memory usage: 1994.59 MB\n",
      "\n",
      "Processing chunk 7\n",
      "Chunk 7 Batch 0 Loss 6.0532\n",
      "Memory usage: 1994.59 MB\n",
      "Epoch 1 Average Loss: 7.4107\n",
      "\n",
      "Epoch 2/15\n",
      "\n",
      "Processing chunk 1\n",
      "Chunk 1 Batch 0 Loss 6.0840\n",
      "Chunk 1 Batch 50 Loss 5.8316\n",
      "Memory usage: 1994.59 MB\n",
      "\n",
      "Processing chunk 2\n",
      "Chunk 2 Batch 0 Loss 5.8828\n",
      "Chunk 2 Batch 50 Loss 5.6586\n",
      "Memory usage: 1994.59 MB\n",
      "\n",
      "Processing chunk 3\n",
      "Chunk 3 Batch 0 Loss 5.3408\n",
      "Chunk 3 Batch 50 Loss 5.4304\n",
      "Memory usage: 1994.59 MB\n",
      "\n",
      "Processing chunk 4\n",
      "Chunk 4 Batch 0 Loss 5.3577\n",
      "Chunk 4 Batch 50 Loss 5.2599\n",
      "Memory usage: 1994.59 MB\n",
      "\n",
      "Processing chunk 5\n",
      "Chunk 5 Batch 0 Loss 5.1726\n",
      "Chunk 5 Batch 50 Loss 5.1499\n",
      "Memory usage: 1994.59 MB\n",
      "\n",
      "Processing chunk 6\n",
      "Chunk 6 Batch 0 Loss 4.8975\n",
      "Chunk 6 Batch 50 Loss 5.2034\n",
      "Memory usage: 1994.59 MB\n",
      "\n",
      "Processing chunk 7\n",
      "Chunk 7 Batch 0 Loss 4.9286\n",
      "Memory usage: 1994.59 MB\n",
      "Epoch 2 Average Loss: 5.3963\n",
      "\n",
      "Epoch 3/15\n",
      "\n",
      "Processing chunk 1\n",
      "Chunk 1 Batch 0 Loss 5.2208\n",
      "Chunk 1 Batch 50 Loss 4.8251\n",
      "Memory usage: 1994.59 MB\n",
      "\n",
      "Processing chunk 2\n",
      "Chunk 2 Batch 0 Loss 4.9510\n",
      "Chunk 2 Batch 50 Loss 4.9559\n",
      "Memory usage: 1994.59 MB\n",
      "\n",
      "Processing chunk 3\n",
      "Chunk 3 Batch 0 Loss 4.9119\n",
      "Chunk 3 Batch 50 Loss 4.7419\n",
      "Memory usage: 1994.59 MB\n",
      "\n",
      "Processing chunk 4\n",
      "Chunk 4 Batch 0 Loss 4.4935\n",
      "Chunk 4 Batch 50 Loss 4.5077\n",
      "Memory usage: 1994.62 MB\n",
      "\n",
      "Processing chunk 5\n",
      "Chunk 5 Batch 0 Loss 4.5817\n",
      "Chunk 5 Batch 50 Loss 4.1388\n",
      "Memory usage: 1994.62 MB\n",
      "\n",
      "Processing chunk 6\n",
      "Chunk 6 Batch 0 Loss 4.4305\n",
      "Chunk 6 Batch 50 Loss 4.3814\n",
      "Memory usage: 1994.62 MB\n",
      "\n",
      "Processing chunk 7\n",
      "Chunk 7 Batch 0 Loss 4.6487\n",
      "Memory usage: 1994.62 MB\n",
      "Epoch 3 Average Loss: 4.6087\n",
      "\n",
      "Epoch 4/15\n",
      "\n",
      "Processing chunk 1\n",
      "Chunk 1 Batch 0 Loss 4.5255\n",
      "Chunk 1 Batch 50 Loss 4.3639\n",
      "Memory usage: 1994.62 MB\n",
      "\n",
      "Processing chunk 2\n",
      "Chunk 2 Batch 0 Loss 4.3675\n",
      "Chunk 2 Batch 50 Loss 4.4022\n",
      "Memory usage: 1994.73 MB\n",
      "\n",
      "Processing chunk 3\n",
      "Chunk 3 Batch 0 Loss 4.2990\n",
      "Chunk 3 Batch 50 Loss 4.4885\n",
      "Memory usage: 1994.73 MB\n",
      "\n",
      "Processing chunk 4\n",
      "Chunk 4 Batch 0 Loss 3.9617\n",
      "Chunk 4 Batch 50 Loss 3.9125\n",
      "Memory usage: 1994.73 MB\n",
      "\n",
      "Processing chunk 5\n",
      "Chunk 5 Batch 0 Loss 4.0510\n",
      "Chunk 5 Batch 50 Loss 3.7889\n",
      "Memory usage: 1994.73 MB\n",
      "\n",
      "Processing chunk 6\n",
      "Chunk 6 Batch 0 Loss 3.7641\n",
      "Chunk 6 Batch 50 Loss 3.9436\n",
      "Memory usage: 1994.92 MB\n",
      "\n",
      "Processing chunk 7\n",
      "Chunk 7 Batch 0 Loss 4.3970\n",
      "Memory usage: 1994.92 MB\n",
      "Epoch 4 Average Loss: 4.1645\n",
      "\n",
      "Epoch 5/15\n",
      "\n",
      "Processing chunk 1\n",
      "Chunk 1 Batch 0 Loss 4.3323\n",
      "Chunk 1 Batch 50 Loss 4.2986\n",
      "Memory usage: 1995.07 MB\n",
      "\n",
      "Processing chunk 2\n",
      "Chunk 2 Batch 0 Loss 4.0437\n",
      "Chunk 2 Batch 50 Loss 3.9932\n",
      "Memory usage: 1995.07 MB\n",
      "\n",
      "Processing chunk 3\n",
      "Chunk 3 Batch 0 Loss 4.0223\n",
      "Chunk 3 Batch 50 Loss 4.0441\n",
      "Memory usage: 1995.07 MB\n",
      "\n",
      "Processing chunk 4\n",
      "Chunk 4 Batch 0 Loss 3.4608\n",
      "Chunk 4 Batch 50 Loss 3.9431\n",
      "Memory usage: 1995.07 MB\n",
      "\n",
      "Processing chunk 5\n",
      "Chunk 5 Batch 0 Loss 3.6736\n",
      "Chunk 5 Batch 50 Loss 3.6386\n",
      "Memory usage: 1995.09 MB\n",
      "\n",
      "Processing chunk 6\n",
      "Chunk 6 Batch 0 Loss 4.1298\n",
      "Chunk 6 Batch 50 Loss 3.9175\n",
      "Memory usage: 1995.30 MB\n",
      "\n",
      "Processing chunk 7\n",
      "Chunk 7 Batch 0 Loss 3.6598\n",
      "Memory usage: 1995.42 MB\n",
      "Epoch 5 Average Loss: 3.8550\n",
      "\n",
      "Epoch 6/15\n",
      "\n",
      "Processing chunk 1\n",
      "Chunk 1 Batch 0 Loss 4.2845\n",
      "Chunk 1 Batch 50 Loss 3.7316\n",
      "Memory usage: 1995.42 MB\n",
      "\n",
      "Processing chunk 2\n",
      "Chunk 2 Batch 0 Loss 3.7648\n",
      "Chunk 2 Batch 50 Loss 3.8170\n",
      "Memory usage: 1995.42 MB\n",
      "\n",
      "Processing chunk 3\n",
      "Chunk 3 Batch 0 Loss 3.7327\n",
      "Chunk 3 Batch 50 Loss 3.6132\n",
      "Memory usage: 1995.42 MB\n",
      "\n",
      "Processing chunk 4\n",
      "Chunk 4 Batch 0 Loss 3.7513\n",
      "Chunk 4 Batch 50 Loss 3.6477\n",
      "Memory usage: 1995.51 MB\n",
      "\n",
      "Processing chunk 5\n",
      "Chunk 5 Batch 0 Loss 3.4581\n",
      "Chunk 5 Batch 50 Loss 3.4186\n",
      "Memory usage: 1995.51 MB\n",
      "\n",
      "Processing chunk 6\n",
      "Chunk 6 Batch 0 Loss 3.5133\n",
      "Chunk 6 Batch 50 Loss 3.2899\n",
      "Memory usage: 1995.51 MB\n",
      "\n",
      "Processing chunk 7\n",
      "Chunk 7 Batch 0 Loss 3.7247\n",
      "Memory usage: 1995.51 MB\n",
      "Epoch 6 Average Loss: 3.5984\n",
      "\n",
      "Epoch 7/15\n",
      "\n",
      "Processing chunk 1\n",
      "Chunk 1 Batch 0 Loss 3.7157\n",
      "Chunk 1 Batch 50 Loss 3.5047\n",
      "Memory usage: 1995.52 MB\n",
      "\n",
      "Processing chunk 2\n",
      "Chunk 2 Batch 0 Loss 3.8325\n",
      "Chunk 2 Batch 50 Loss 3.4605\n",
      "Memory usage: 1995.52 MB\n",
      "\n",
      "Processing chunk 3\n",
      "Chunk 3 Batch 0 Loss 3.2200\n",
      "Chunk 3 Batch 50 Loss 3.4494\n",
      "Memory usage: 1995.52 MB\n",
      "\n",
      "Processing chunk 4\n",
      "Chunk 4 Batch 0 Loss 3.4619\n",
      "Chunk 4 Batch 50 Loss 3.2632\n",
      "Memory usage: 1995.52 MB\n",
      "\n",
      "Processing chunk 5\n",
      "Chunk 5 Batch 0 Loss 3.2587\n",
      "Chunk 5 Batch 50 Loss 3.2762\n",
      "Memory usage: 1995.52 MB\n",
      "\n",
      "Processing chunk 6\n",
      "Chunk 6 Batch 0 Loss 3.5635\n",
      "Chunk 6 Batch 50 Loss 3.4599\n",
      "Memory usage: 1995.52 MB\n",
      "\n",
      "Processing chunk 7\n",
      "Chunk 7 Batch 0 Loss 3.1780\n",
      "Memory usage: 1995.52 MB\n",
      "Epoch 7 Average Loss: 3.3844\n",
      "\n",
      "Epoch 8/15\n",
      "\n",
      "Processing chunk 1\n",
      "Chunk 1 Batch 0 Loss 3.8285\n",
      "Chunk 1 Batch 50 Loss 3.4005\n",
      "Memory usage: 1995.52 MB\n",
      "\n",
      "Processing chunk 2\n",
      "Chunk 2 Batch 0 Loss 3.4849\n",
      "Chunk 2 Batch 50 Loss 3.3142\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 3\n",
      "Chunk 3 Batch 0 Loss 3.5776\n",
      "Chunk 3 Batch 50 Loss 3.2284\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 4\n",
      "Chunk 4 Batch 0 Loss 2.9091\n",
      "Chunk 4 Batch 50 Loss 3.2627\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 5\n",
      "Chunk 5 Batch 0 Loss 3.1393\n",
      "Chunk 5 Batch 50 Loss 3.0693\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 6\n",
      "Chunk 6 Batch 0 Loss 3.0309\n",
      "Chunk 6 Batch 50 Loss 3.0638\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 7\n",
      "Chunk 7 Batch 0 Loss 3.3241\n",
      "Memory usage: 1995.68 MB\n",
      "Epoch 8 Average Loss: 3.1998\n",
      "\n",
      "Epoch 9/15\n",
      "\n",
      "Processing chunk 1\n",
      "Chunk 1 Batch 0 Loss 3.6494\n",
      "Chunk 1 Batch 50 Loss 3.0223\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 2\n",
      "Chunk 2 Batch 0 Loss 3.2153\n",
      "Chunk 2 Batch 50 Loss 3.1231\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 3\n",
      "Chunk 3 Batch 0 Loss 3.3106\n",
      "Chunk 3 Batch 50 Loss 3.5025\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 4\n",
      "Chunk 4 Batch 0 Loss 3.0056\n",
      "Chunk 4 Batch 50 Loss 2.9679\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 5\n",
      "Chunk 5 Batch 0 Loss 2.9218\n",
      "Chunk 5 Batch 50 Loss 2.7771\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 6\n",
      "Chunk 6 Batch 0 Loss 3.0290\n",
      "Chunk 6 Batch 50 Loss 2.9604\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 7\n",
      "Chunk 7 Batch 0 Loss 2.8416\n",
      "Memory usage: 1995.68 MB\n",
      "Epoch 9 Average Loss: 3.0392\n",
      "\n",
      "Epoch 10/15\n",
      "\n",
      "Processing chunk 1\n",
      "Chunk 1 Batch 0 Loss 3.5556\n",
      "Chunk 1 Batch 50 Loss 3.1326\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 2\n",
      "Chunk 2 Batch 0 Loss 3.2921\n",
      "Chunk 2 Batch 50 Loss 3.1876\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 3\n",
      "Chunk 3 Batch 0 Loss 3.3723\n",
      "Chunk 3 Batch 50 Loss 2.9661\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 4\n",
      "Chunk 4 Batch 0 Loss 2.8245\n",
      "Chunk 4 Batch 50 Loss 2.8373\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 5\n",
      "Chunk 5 Batch 0 Loss 2.8282\n",
      "Chunk 5 Batch 50 Loss 2.9263\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 6\n",
      "Chunk 6 Batch 0 Loss 2.7340\n",
      "Chunk 6 Batch 50 Loss 2.7266\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 7\n",
      "Chunk 7 Batch 0 Loss 2.7838\n",
      "Memory usage: 1995.68 MB\n",
      "Epoch 10 Average Loss: 2.9073\n",
      "\n",
      "Epoch 11/15\n",
      "\n",
      "Processing chunk 1\n",
      "Chunk 1 Batch 0 Loss 3.5496\n",
      "Chunk 1 Batch 50 Loss 2.8035\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 2\n",
      "Chunk 2 Batch 0 Loss 2.9991\n",
      "Chunk 2 Batch 50 Loss 3.0161\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 3\n",
      "Chunk 3 Batch 0 Loss 2.7358\n",
      "Chunk 3 Batch 50 Loss 2.9343\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 4\n",
      "Chunk 4 Batch 0 Loss 2.6866\n",
      "Chunk 4 Batch 50 Loss 2.8263\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 5\n",
      "Chunk 5 Batch 0 Loss 2.7350\n",
      "Chunk 5 Batch 50 Loss 2.5385\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 6\n",
      "Chunk 6 Batch 0 Loss 2.8487\n",
      "Chunk 6 Batch 50 Loss 2.4510\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 7\n",
      "Chunk 7 Batch 0 Loss 2.6745\n",
      "Memory usage: 1995.68 MB\n",
      "Epoch 11 Average Loss: 2.7547\n",
      "\n",
      "Epoch 12/15\n",
      "\n",
      "Processing chunk 1\n",
      "Chunk 1 Batch 0 Loss 3.4283\n",
      "Chunk 1 Batch 50 Loss 2.9010\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 2\n",
      "Chunk 2 Batch 0 Loss 2.9809\n",
      "Chunk 2 Batch 50 Loss 2.5485\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 3\n",
      "Chunk 3 Batch 0 Loss 2.7465\n",
      "Chunk 3 Batch 50 Loss 2.6590\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 4\n",
      "Chunk 4 Batch 0 Loss 2.7203\n",
      "Chunk 4 Batch 50 Loss 2.3389\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 5\n",
      "Chunk 5 Batch 0 Loss 2.5767\n",
      "Chunk 5 Batch 50 Loss 2.1606\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 6\n",
      "Chunk 6 Batch 0 Loss 2.5490\n",
      "Chunk 6 Batch 50 Loss 2.3382\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 7\n",
      "Chunk 7 Batch 0 Loss 2.4494\n",
      "Memory usage: 1995.68 MB\n",
      "Epoch 12 Average Loss: 2.5554\n",
      "\n",
      "Epoch 13/15\n",
      "\n",
      "Processing chunk 1\n",
      "Chunk 1 Batch 0 Loss 3.2432\n",
      "Chunk 1 Batch 50 Loss 2.8405\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 2\n",
      "Chunk 2 Batch 0 Loss 2.8579\n",
      "Chunk 2 Batch 50 Loss 2.4774\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 3\n",
      "Chunk 3 Batch 0 Loss 2.5796\n",
      "Chunk 3 Batch 50 Loss 2.4177\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 4\n",
      "Chunk 4 Batch 0 Loss 2.3704\n",
      "Chunk 4 Batch 50 Loss 2.2549\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 5\n",
      "Chunk 5 Batch 0 Loss 2.3616\n",
      "Chunk 5 Batch 50 Loss 2.2422\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 6\n",
      "Chunk 6 Batch 0 Loss 2.0996\n",
      "Chunk 6 Batch 50 Loss 2.0794\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 7\n",
      "Chunk 7 Batch 0 Loss 2.0733\n",
      "Memory usage: 1995.68 MB\n",
      "Epoch 13 Average Loss: 2.3525\n",
      "\n",
      "Epoch 14/15\n",
      "\n",
      "Processing chunk 1\n",
      "Chunk 1 Batch 0 Loss 3.0825\n",
      "Chunk 1 Batch 50 Loss 2.6060\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 2\n",
      "Chunk 2 Batch 0 Loss 2.6867\n",
      "Chunk 2 Batch 50 Loss 2.3128\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 3\n",
      "Chunk 3 Batch 0 Loss 2.3996\n",
      "Chunk 3 Batch 50 Loss 2.3707\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 4\n",
      "Chunk 4 Batch 0 Loss 2.3546\n",
      "Chunk 4 Batch 50 Loss 2.2507\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 5\n",
      "Chunk 5 Batch 0 Loss 1.9767\n",
      "Chunk 5 Batch 50 Loss 2.0178\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 6\n",
      "Chunk 6 Batch 0 Loss 2.2552\n",
      "Chunk 6 Batch 50 Loss 1.8920\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 7\n",
      "Chunk 7 Batch 0 Loss 2.0707\n",
      "Memory usage: 1995.68 MB\n",
      "Epoch 14 Average Loss: 2.1680\n",
      "\n",
      "Epoch 15/15\n",
      "\n",
      "Processing chunk 1\n",
      "Chunk 1 Batch 0 Loss 2.7896\n",
      "Chunk 1 Batch 50 Loss 2.5606\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 2\n",
      "Chunk 2 Batch 0 Loss 2.2042\n",
      "Chunk 2 Batch 50 Loss 2.3300\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 3\n",
      "Chunk 3 Batch 0 Loss 2.0914\n",
      "Chunk 3 Batch 50 Loss 1.8926\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 4\n",
      "Chunk 4 Batch 0 Loss 2.0089\n",
      "Chunk 4 Batch 50 Loss 1.6984\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 5\n",
      "Chunk 5 Batch 0 Loss 1.8829\n",
      "Chunk 5 Batch 50 Loss 1.8290\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 6\n",
      "Chunk 6 Batch 0 Loss 1.8480\n",
      "Chunk 6 Batch 50 Loss 1.6900\n",
      "Memory usage: 1995.68 MB\n",
      "\n",
      "Processing chunk 7\n",
      "Chunk 7 Batch 0 Loss 1.5009\n",
      "Memory usage: 1995.68 MB\n",
      "Epoch 15 Average Loss: 1.9892\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "import torch\n",
    "import nltk\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Positional encoding layer\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        # Apply sin to even indices in the array\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # Apply cos to odd indices in the array\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Get the sequence length of the input\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "\n",
    "        # Slice the positional encoding to match the input sequence length\n",
    "        pos_encoding_slice = self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        return inputs + pos_encoding_slice\n",
    "\n",
    "# Scaled dot product attention\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # query shape: (batch_size, num_heads, seq_len_q, depth)\n",
    "    # key shape: (batch_size, num_heads, seq_len_k, depth)\n",
    "    # value shape: (batch_size, num_heads, seq_len_v, depth)\n",
    "    # mask shape: (batch_size, 1, seq_len_q, seq_len_k) or (batch_size, 1, 1, seq_len_k)\n",
    "\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # Scale matmul_qk\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # Add mask if provided\n",
    "    if mask is not None:\n",
    "        # Ensure mask has the right shape for broadcasting\n",
    "        mask = tf.cast(mask, dtype=logits.dtype)\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # Softmax is applied to the last axis (seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # Computing the attention output\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Multi-head attention layer\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Linear layers\n",
    "        query = self.query_dense(query)  # (batch_size, seq_len_q, d_model)\n",
    "        key = self.key_dense(key)        # (batch_size, seq_len_k, d_model)\n",
    "        value = self.value_dense(value)  # (batch_size, seq_len_v, d_model)\n",
    "\n",
    "        # Split heads\n",
    "        query = self.split_heads(query, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        key = self.split_heads(key, batch_size)      # (batch_size, num_heads, seq_len_k, depth)\n",
    "        value = self.split_heads(value, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        # Transpose to get back to (batch_size, seq_len_q, num_heads, depth)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # Concatenate heads\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                    (batch_size, -1, self.d_model))\n",
    "\n",
    "        # Final linear layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Point wise feed forward network\n",
    "class PointWiseFeedForwardNetwork(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, name=\"ffn\"):\n",
    "        super(PointWiseFeedForwardNetwork, self).__init__(name=name)\n",
    "        self.dense_1 = tf.keras.layers.Dense(units=dff, activation='relu')\n",
    "        self.dense_2 = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense_1(inputs)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        return outputs\n",
    "\n",
    "# Encoder layer\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1, name=\"encoder_layer\"):\n",
    "        super(EncoderLayer, self).__init__(name=name)\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PointWiseFeedForwardNetwork(d_model, dff)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        x = inputs  # Unpack the input tensor\n",
    "        attn_output = self.mha({'query': x, 'key': x, 'value': x, 'mask': mask})\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "# Decoder layer\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1, name=\"decoder_layer\"):\n",
    "        super(DecoderLayer, self).__init__(name=name)\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PointWiseFeedForwardNetwork(d_model, dff)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x, enc_output, look_ahead_mask, padding_mask = inputs\n",
    "        attn1 = self.mha1({'query': x, 'key': x, 'value': x, 'mask': look_ahead_mask})\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        attn2 = self.mha2({'query': out1, 'key': enc_output, 'value': enc_output, 'mask': padding_mask})\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        return out3\n",
    "\n",
    "# Encoder\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1, name=\"encoder\"):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        # Cast inputs to int32 if needed\n",
    "        inputs = tf.cast(inputs, tf.int32)\n",
    "\n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        x = self.embedding(inputs)\n",
    "        x = tf.cast(x, tf.float32)\n",
    "\n",
    "        # Scale embedding\n",
    "        x *= tf.math.sqrt(self.d_model)\n",
    "        x += self.pos_encoding(x)\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Decoder\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1, name=\"decoder\"):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, enc_output, look_ahead_mask, padding_mask, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i]([x, enc_output, look_ahead_mask, padding_mask], training=training)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Transformer\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                             input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                             target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Unpack the inputs tuple\n",
    "        inp, tar, enc_padding_mask, look_ahead_mask, dec_padding_mask = inputs\n",
    "\n",
    "        # Cast all inputs to appropriate types\n",
    "        inp = tf.cast(inp, tf.int32)\n",
    "        tar = tf.cast(tar, tf.int32)\n",
    "        enc_padding_mask = tf.cast(enc_padding_mask, tf.float32)\n",
    "        look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "        dec_padding_mask = tf.cast(dec_padding_mask, tf.float32)\n",
    "\n",
    "        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)\n",
    "        dec_output = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask, training=training)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        return final_output\n",
    "\n",
    "# Preprocessing the data\n",
    "def load_data(english_dir, urdu_dir):\n",
    "    try:\n",
    "        english_files = sorted(os.listdir(english_dir))\n",
    "        urdu_files = sorted(os.listdir(urdu_dir))\n",
    "\n",
    "        if len(english_files) == 0 or len(urdu_files) == 0:\n",
    "            raise ValueError(\"No files found in the directories\")\n",
    "\n",
    "        english_data = []\n",
    "        urdu_data = []\n",
    "\n",
    "        for eng_file, urdu_file in zip(english_files, urdu_files):\n",
    "            try:\n",
    "                with open(os.path.join(english_dir, eng_file), 'r', encoding='utf-8') as f:\n",
    "                    # Split the text into sentences and decode if necessary\n",
    "                    eng_sentences = f.read().strip().split('\\n')\n",
    "                    eng_sentences = [sent.strip() for sent in eng_sentences if sent.strip()]\n",
    "                    english_data.extend(eng_sentences)\n",
    "\n",
    "                with open(os.path.join(urdu_dir, urdu_file), 'r', encoding='utf-8') as f:\n",
    "                    # Split the text into sentences and decode if necessary\n",
    "                    urdu_sentences = f.read().strip().split('\\n')\n",
    "                    urdu_sentences = [sent.strip() for sent in urdu_sentences if sent.strip()]\n",
    "                    urdu_data.extend(urdu_sentences)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading files {eng_file} and {urdu_file}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Ensure all data is in string format\n",
    "        english_data = [str(text) for text in english_data]\n",
    "        urdu_data = [str(text) for text in urdu_data]\n",
    "\n",
    "        if len(english_data) == 0 or len(urdu_data) == 0:\n",
    "            raise ValueError(\"No valid data loaded from files\")\n",
    "\n",
    "        if len(english_data) != len(urdu_data):\n",
    "            min_len = min(len(english_data), len(urdu_data))\n",
    "            english_data = english_data[:min_len]\n",
    "            urdu_data = urdu_data[:min_len]\n",
    "            print(f\"Warning: Trimmed data to ensure equal lengths. Using {min_len} sentence pairs.\")\n",
    "\n",
    "        print(f\"Loaded {len(english_data)} English sentences and {len(urdu_data)} Urdu sentences\")\n",
    "\n",
    "        # Print a few examples to verify the data\n",
    "        print(\"\\nFirst 3 sentence pairs:\")\n",
    "        for i in range(min(3, len(english_data))):\n",
    "            print(f\"\\nEnglish: {english_data[i]}\")\n",
    "            print(f\"Urdu: {urdu_data[i]}\")\n",
    "\n",
    "        return english_data, urdu_data\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error loading data: {str(e)}\")\n",
    "\n",
    "english_dir = \"drive/MyDrive/Colab Datasets/umc005-corpus/quran/data-en\"\n",
    "urdu_dir = \"drive/MyDrive/Colab Datasets/umc005-corpus/quran/data-ur\"\n",
    "english_data, urdu_data = load_data(english_dir, urdu_dir)\n",
    "\n",
    "# Creating the vocabulary with special tokens\n",
    "def create_vocabulary(data):\n",
    "    vocab = set()\n",
    "    for sentence in data:\n",
    "        vocab.update(sentence.split())\n",
    "    # Add special tokens\n",
    "    vocab.update(['<start>', '<end>', '<pad>'])\n",
    "    return vocab\n",
    "\n",
    "# Initialize tokenizers with special tokens\n",
    "english_vocab = create_vocabulary(english_data)\n",
    "urdu_vocab = create_vocabulary(urdu_data)\n",
    "\n",
    "english_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "urdu_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "\n",
    "# Add special tokens to the vocabulary\n",
    "english_tokenizer.fit_on_texts(['<start> ' + ' '.join(english_vocab) + ' <end> <pad>'])\n",
    "urdu_tokenizer.fit_on_texts(['<start> ' + ' '.join(urdu_vocab) + ' <end> <pad>'])\n",
    "\n",
    "# Generating masks\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    # Create a lower triangular matrix\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    # Add batch dimension and head dimension\n",
    "    mask = mask[tf.newaxis, tf.newaxis, :, :]\n",
    "    return mask\n",
    "\n",
    "max_sequence_length = 130  # Desired maximum length\n",
    "\n",
    "def preprocess_sequences(sequences, tokenizer, max_length):\n",
    "    # Convert bytes to strings if necessary\n",
    "    sequences = [seq.decode('utf-8') if isinstance(seq, bytes) else seq for seq in sequences]\n",
    "\n",
    "    # Clean and normalize the text\n",
    "    sequences = [' '.join(seq.lower().split()) for seq in sequences]\n",
    "\n",
    "    # Add start and end tokens\n",
    "    sequences = ['<start> ' + seq + ' <end>' for seq in sequences]\n",
    "\n",
    "    # Convert to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(sequences)\n",
    "\n",
    "    # Pad sequences\n",
    "    sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequences,\n",
    "        maxlen=max_length,\n",
    "        padding='post',\n",
    "        truncating='post'\n",
    "    )\n",
    "    return sequences\n",
    "\n",
    "# Instantiating the model\n",
    "num_layers = 4\n",
    "d_model = 256\n",
    "dff = 1024\n",
    "num_heads = 4\n",
    "input_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(urdu_tokenizer.word_index) + 1\n",
    "dropout_rate = 0.1\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    pe_input=1000,\n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "batch_size = 16\n",
    "epochs = 15\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'transformer_checkpoint.keras',\n",
    "    monitor='loss',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# Add these callbacks to the training\n",
    "callbacks = [early_stopping, model_checkpoint]\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"Calculate loss with proper type handling\"\"\"\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(tf.cast(self.d_model, tf.float32)) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"d_model\": self.d_model,\n",
    "            \"warmup_steps\": self.warmup_steps\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "# Model compilation\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.98,\n",
    "    epsilon=1e-9,\n",
    "    clipnorm=1.0  # Add gradient clipping\n",
    ")\n",
    "\n",
    "transformer.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_function,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Clear memory\n",
    "def clear_memory():\n",
    "    \"\"\"Enhanced memory clearing function with GPU support\"\"\"\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Clear GPU memory if available\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Reset GPU memory\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.reset_memory_stats(gpu)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Force garbage collection\n",
    "    for _ in range(3):\n",
    "        gc.collect()\n",
    "\n",
    "    # Print memory usage information\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process()\n",
    "        print(f\"Memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "# Create tf.data.Dataset for better performance\n",
    "def create_dataset(eng_data, urdu_data, batch_size):\n",
    "    \"\"\"Create a tf.data.Dataset with proper shapes and types.\"\"\"\n",
    "    if len(eng_data) < batch_size:\n",
    "        print(f\"Warning: Dataset size ({len(eng_data)}) is smaller than batch_size ({batch_size})\")\n",
    "        batch_size = max(1, len(eng_data) // 2)\n",
    "        print(f\"Adjusted batch_size to: {batch_size}\")\n",
    "\n",
    "    # Convert sequences to string tensors\n",
    "    eng_tensor = tf.convert_to_tensor([str(text) for text in eng_data], dtype=tf.string)\n",
    "    urdu_tensor = tf.convert_to_tensor([str(text) for text in urdu_data], dtype=tf.string)\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_tensor, urdu_tensor))\n",
    "    dataset = dataset.shuffle(len(eng_data))\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset, batch_size\n",
    "\n",
    "# Add this function for chunk-based data processing\n",
    "def process_data_in_chunks(english_data, urdu_data, chunk_size=1000):\n",
    "    \"\"\"Process data in chunks to manage memory better.\"\"\"\n",
    "    total_samples = len(english_data)\n",
    "    num_chunks = (total_samples + chunk_size - 1) // chunk_size\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, total_samples)\n",
    "\n",
    "        # Get current chunk\n",
    "        eng_chunk = english_data[start_idx:end_idx]\n",
    "        urdu_chunk = urdu_data[start_idx:end_idx]\n",
    "\n",
    "        yield eng_chunk, urdu_chunk\n",
    "\n",
    "@tf.function\n",
    "def train_step(transformer, inputs, target):\n",
    "    \"\"\"Single training step with GPU optimization\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Unpack inputs and ensure proper shapes\n",
    "        eng_padded, dec_input, enc_padding_mask, combined_mask, dec_padding_mask = inputs\n",
    "\n",
    "        # Create input tuple for transformer\n",
    "        transformer_inputs = (\n",
    "            tf.cast(eng_padded, tf.int32),\n",
    "            tf.cast(dec_input, tf.int32),\n",
    "            tf.cast(enc_padding_mask, tf.float32),\n",
    "            tf.cast(combined_mask, tf.float32),\n",
    "            tf.cast(dec_padding_mask, tf.float32)\n",
    "        )\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = transformer(transformer_inputs, training=True)\n",
    "\n",
    "        # Ensure target has the correct shape and type\n",
    "        target = tf.cast(target, tf.int32)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(target, predictions)\n",
    "\n",
    "        if loss is None:\n",
    "            tf.print(\"Warning: Loss is None\")\n",
    "            return 0.0\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "\n",
    "    # Clip gradients to prevent exploding gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, clip_norm=1.0)\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train_model_with_chunks(transformer, english_data, urdu_data, epochs, batch_size, chunk_size=1000):\n",
    "    \"\"\"Train the model using chunk-based processing.\"\"\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "        total_loss = 0.0\n",
    "        total_batches = 0\n",
    "\n",
    "        for chunk_num, (eng_chunk, urdu_chunk) in enumerate(process_data_in_chunks(english_data, urdu_data, chunk_size)):\n",
    "            print(f'\\nProcessing chunk {chunk_num + 1}')\n",
    "\n",
    "            try:\n",
    "                # Create dataset for current chunk\n",
    "                chunk_dataset, adjusted_batch_size = create_dataset(eng_chunk, urdu_chunk, batch_size)\n",
    "\n",
    "                for batch, (eng, urdu) in enumerate(chunk_dataset):\n",
    "                    try:\n",
    "                        with tf.device('/GPU:0'):\n",
    "                            # Print shapes for debugging\n",
    "                            # print(f\"Input shapes - eng: {eng.shape}, urdu: {urdu.shape}\")\n",
    "\n",
    "                            # Preprocess sequences\n",
    "                            eng_padded = preprocess_sequences(eng.numpy(), english_tokenizer, max_sequence_length)\n",
    "                            urdu_padded = preprocess_sequences(urdu.numpy(), urdu_tokenizer, max_sequence_length)\n",
    "\n",
    "                            # print(f\"Preprocessed shapes - eng_padded: {eng_padded.shape}, urdu_padded: {urdu_padded.shape}\")\n",
    "\n",
    "                            # Create masks\n",
    "                            enc_padding_mask = create_padding_mask(eng_padded)\n",
    "                            dec_padding_mask = create_padding_mask(eng_padded)\n",
    "                            dec_input = urdu_padded[:, :-1]  # Remove last token\n",
    "                            dec_target = urdu_padded[:, 1:]  # Remove first token\n",
    "\n",
    "                            look_ahead_mask = create_look_ahead_mask(tf.shape(dec_input)[1])\n",
    "                            combined_mask = tf.maximum(\n",
    "                                create_padding_mask(dec_input),\n",
    "                                look_ahead_mask\n",
    "                            )\n",
    "\n",
    "                            # Print shapes for debugging\n",
    "                            # print(f\"Mask shapes - enc_padding_mask: {enc_padding_mask.shape}, \"\n",
    "                            #       f\"combined_mask: {combined_mask.shape}, \"\n",
    "                            #       f\"dec_padding_mask: {dec_padding_mask.shape}\")\n",
    "\n",
    "                            # Prepare inputs\n",
    "                            inputs = (\n",
    "                                eng_padded,\n",
    "                                dec_input,\n",
    "                                enc_padding_mask,\n",
    "                                combined_mask,\n",
    "                                dec_padding_mask\n",
    "                            )\n",
    "\n",
    "                            # Training step\n",
    "                            batch_loss = train_step(transformer, inputs, dec_target)\n",
    "\n",
    "                            if batch_loss is None:\n",
    "                                raise ValueError(\"Loss value is None\")\n",
    "\n",
    "                            total_loss += float(batch_loss)\n",
    "                            total_batches += 1\n",
    "\n",
    "                            if batch % 50 == 0:\n",
    "                                print(f'Chunk {chunk_num + 1} Batch {batch} Loss {float(batch_loss):.4f}')\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing batch {batch}: {str(e)}\")\n",
    "                        print(f\"Batch shapes - eng: {eng.shape}, urdu: {urdu.shape}\")\n",
    "                        continue\n",
    "\n",
    "                # Save checkpoint after each chunk\n",
    "                # if chunk_num % 5 == 0:\n",
    "                    # checkpoint_path = f'transformer_checkpoint_epoch_{epoch + 1}_chunk_{chunk_num}.keras'\n",
    "                    # transformer.save(checkpoint_path)\n",
    "                    # print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunk {chunk_num}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "            clear_memory()\n",
    "\n",
    "        # Calculate and print average loss for the epoch\n",
    "        if total_batches > 0:\n",
    "            avg_loss = total_loss / total_batches\n",
    "            print(f'Epoch {epoch + 1} Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "            # Save epoch checkpoint\n",
    "            # epoch_checkpoint_path = f'transformer_checkpoint_epoch_{epoch + 1}.keras'\n",
    "            # transformer.save(epoch_checkpoint_path)\n",
    "            # print(f\"Saved epoch checkpoint: {epoch_checkpoint_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: No batches processed in epoch {epoch + 1}\")\n",
    "\n",
    "    return transformer\n",
    "\n",
    "# Create dataset\n",
    "train_dataset, adjusted_batch_size = create_dataset(english_data, urdu_data, batch_size)\n",
    "if not isinstance(train_dataset, tf.data.Dataset):\n",
    "    raise ValueError(\"train_dataset must be a tf.data.Dataset instance\")\n",
    "\n",
    "sequence_length = 0\n",
    "\n",
    "# Replace the existing training loop with this\n",
    "chunk_size = 1000\n",
    "trained_transformer = train_model_with_chunks(\n",
    "    transformer=transformer,\n",
    "    english_data=english_data,\n",
    "    urdu_data=urdu_data,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    chunk_size=chunk_size\n",
    ")\n",
    "\n",
    "# Create dummy input that matches the expected input shapes\n",
    "dummy_inp = tf.random.uniform((1, sequence_length), dtype=tf.int32, maxval=input_vocab_size)  # Encoder input\n",
    "dummy_tar = tf.random.uniform((1, sequence_length), dtype=tf.int32, maxval=target_vocab_size)  # Decoder input\n",
    "\n",
    "# Create a dummy input that matches the expected input shape\n",
    "# (batch_size, sequence_length) for the source input, and\n",
    "# a similar shape for the target input (e.g., padding masks, etc.)\n",
    "dummy_inp = tf.random.uniform((1, sequence_length), dtype=tf.int32, maxval=input_vocab_size)  # Example for encoder input\n",
    "dummy_tar = tf.random.uniform((1, sequence_length), dtype=tf.int32, maxval=target_vocab_size)  # Example for decoder input\n",
    "\n",
    "# Assuming padding masks and look-ahead masks are already handled in the Transformer call,\n",
    "# you can create random tensors for these masks\n",
    "dummy_enc_padding_mask = tf.random.uniform((1, 1, 1, sequence_length), dtype=tf.float32)\n",
    "dummy_look_ahead_mask = tf.random.uniform((1, 1, sequence_length, sequence_length), dtype=tf.float32)\n",
    "dummy_dec_padding_mask = tf.random.uniform((1, 1, 1, sequence_length), dtype=tf.float32)\n",
    "\n",
    "# Pass the dummy input through the model to \"build\" it\n",
    "transformer((dummy_inp, dummy_tar, dummy_enc_padding_mask, dummy_look_ahead_mask, dummy_dec_padding_mask))\n",
    "\n",
    "# Saving the model\n",
    "model_save_path = \"english_to_urdu_transformer.keras\"\n",
    "trained_transformer.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "MkprQLdtH4yg",
    "outputId": "b5fcf004-6fb2-4755-c960-7ad137deef4e"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_afc0e328-c970-43bf-a622-b0c76265880c\", \"english_to_urdu_transformer.keras\", 164072467)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ZQYGriYWNB_3",
    "outputId": "ea7dfd46-23b2-41be-cd21-38e4e4f406fb"
   },
   "outputs": [],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rbBnVkWKHssx",
    "outputId": "c09d0301-14aa-4925-a99a-e687c38f9854"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model on test set...\n",
      "\n",
      "Example 1:\n",
      "English: Yes indeed ! Would that you knew with the knowledge of certitude ( the consequence of greed for wealth and riches and your negligence . Then , lost in the worldly pleasures , you would never forget the Hereafter like this ) .\n",
      "Reference Urdu: ہاں ہاں ! کاش تم مال و زَر کی ہوس اور اپنی غفلت کے انجام کو یقینی علم کے ساتھ جانتے تو دنیا میں کھو کر آخرت کو اس طرح نہ بھولتے ۔\n",
      "Predicted Urdu: اور آپ کا رب ہرگز دنیا کی زندگی میں آخرت کو آخرت کا گھر نہیں دیتا ۔\n",
      "Character-level Precision: 0.8507\n",
      "Character-level Recall: 0.4419\n",
      "Character-level F1: 0.5816\n",
      "\n",
      "Character-level Analysis:\n",
      "Common characters: 19\n",
      "Reference unique characters: 32\n",
      "Prediction unique characters: 20\n",
      "\n",
      "Example 2:\n",
      "English: ( Consequent on your greed ) you shall surely see Hell .\n",
      "Reference Urdu: تم اپنی حرص کے نتیجے میں دوزخ کو ضرور دیکھ کر رہو گے ۔\n",
      "Predicted Urdu: جس دن تم کفر کرتے رہو گے ۔\n",
      "Character-level Precision: 0.9231\n",
      "Character-level Recall: 0.4444\n",
      "Character-level F1: 0.6000\n",
      "\n",
      "Character-level Analysis:\n",
      "Common characters: 13\n",
      "Reference unique characters: 23\n",
      "Prediction unique characters: 15\n",
      "\n",
      "Example 3:\n",
      "English: Then you shall certainly see it with the eye of certitude .\n",
      "Reference Urdu: پھر تم اسے ضرور یقین کی آنکھ سے دیکھ لو گے ۔\n",
      "Predicted Urdu: پھر تم ضرور ضرور ضرور ضرور ضرور ضرور ضرور ضرور دیکھ کر ضرور ضرور ضرور سواری کرتے ہوئے ضرور ضرور ضرور ضرور ضرور ضرور ضرور ضرور ضرور ضرور ضرور ضرور ضرور ضرور ضرور ہوگا گے ۔\n",
      "Character-level Precision: 0.2059\n",
      "Character-level Recall: 0.7955\n",
      "Character-level F1: 0.3271\n",
      "\n",
      "Character-level Analysis:\n",
      "Common characters: 16\n",
      "Reference unique characters: 20\n",
      "Prediction unique characters: 18\n",
      "\n",
      "Example 4:\n",
      "English: Then on that Day you will certainly be questioned about the bounties ( given to you by Allah and the way you consumed them ) .\n",
      "Reference Urdu: پھر اس دن تم سے اﷲ کی نعمتوں کے بارے میں ضرور پوچھا جائے گا کہ تم نے انہیں کہاں کہاں اور کیسے کیسے خرچ کیا تھا ۔\n",
      "Predicted Urdu: پھر اس دن تم سے اﷲ کی نعمتوں کے بارے میں ضرور پوچھا جائے گا کہ تم عنقریب اس کی نعمتوں کو دیکھ لو گے ۔\n",
      "Character-level Precision: 0.9010\n",
      "Character-level Recall: 0.8125\n",
      "Character-level F1: 0.8545\n",
      "\n",
      "Character-level Analysis:\n",
      "Common characters: 25\n",
      "Reference unique characters: 26\n",
      "Prediction unique characters: 27\n",
      "\n",
      "Example 5:\n",
      "English: By the passing time ( whose rotation bears witness to the affairs of mankind ) .\n",
      "Reference Urdu: زمانہ کی قَسم جس کی گردش انسانی حالات پر گواہ ہے ۔ یا - نمازِ عصر کی قَسم کہ وہ سب نمازوں کا وسط ہے ۔ یا - وقتِ عصر کی قَسم جب دن بھر چمکنے والا سورج خود ڈوبنے کا منظر پیش کرتا ہے ۔ یا - زمانۂ بعثتِ مصطفیٰ صلی اللہ علیہ وآلہ وسلم کی قَسم جو سارے زمانوں کا ماحصل اور مقصود ہے ۔\n",
      "Predicted Urdu: جس دن سب آسمانی کرّے پھٹ جائیں گے ۔\n",
      "Character-level Precision: 0.9143\n",
      "Character-level Recall: 0.1155\n",
      "Character-level F1: 0.2051\n",
      "\n",
      "Character-level Analysis:\n",
      "Common characters: 18\n",
      "Reference unique characters: 40\n",
      "Prediction unique characters: 21\n",
      "\n",
      "Overall Evaluation Metrics:\n",
      "BLEU Score: 0.1216\n",
      "Average Character-level Precision: 0.8466\n",
      "Average Character-level Recall: 0.5309\n",
      "Average Character-level F1: 0.5971\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    \"\"\"Create masks for transformer input\"\"\"\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "def initialize_transformer():\n",
    "    \"\"\"Initialize a new transformer with the same architecture\"\"\"\n",
    "    transformer = Transformer(\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dff=dff,\n",
    "        input_vocab_size=input_vocab_size,\n",
    "        target_vocab_size=target_vocab_size,\n",
    "        pe_input=1000,\n",
    "        pe_target=1000,\n",
    "        rate=dropout_rate\n",
    "    )\n",
    "\n",
    "    # Create dummy input to build the model\n",
    "    dummy_input = tf.random.uniform((1, max_sequence_length), dtype=tf.int32, maxval=input_vocab_size)\n",
    "    dummy_target = tf.random.uniform((1, max_sequence_length), dtype=tf.int32, maxval=target_vocab_size)\n",
    "    dummy_enc_padding_mask = create_padding_mask(dummy_input)\n",
    "    dummy_look_ahead_mask = create_look_ahead_mask(max_sequence_length)\n",
    "    dummy_dec_padding_mask = create_padding_mask(dummy_input)\n",
    "\n",
    "    # Build the model\n",
    "    _ = transformer((\n",
    "        dummy_input,\n",
    "        dummy_target,\n",
    "        dummy_enc_padding_mask,\n",
    "        dummy_look_ahead_mask,\n",
    "        dummy_dec_padding_mask\n",
    "    ), training=False)\n",
    "\n",
    "    return transformer\n",
    "\n",
    "def translate(transformer, sentence, max_length=max_sequence_length):\n",
    "    \"\"\"Translate a single sentence\"\"\"\n",
    "    # Tokenize and preprocess the input sentence\n",
    "    inputs = preprocess_sequences([sentence], english_tokenizer, max_length)\n",
    "\n",
    "    # Initialize decoder input with start token\n",
    "    decoder_input = tf.expand_dims([urdu_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    output = tf.cast(decoder_input, dtype=tf.int32)\n",
    "\n",
    "    for i in range(max_length):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inputs, output)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = transformer(\n",
    "            (inputs, output, enc_padding_mask, combined_mask, dec_padding_mask),\n",
    "            training=False\n",
    "        )\n",
    "\n",
    "        # Get the last token's prediction\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # Return if end token is predicted\n",
    "        if predicted_id == urdu_tokenizer.word_index['<end>']:\n",
    "            break\n",
    "\n",
    "        # Concatenate predicted token to output\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    # Convert token ids to words\n",
    "    output_text = []\n",
    "    for token in output[0].numpy():\n",
    "        word = urdu_tokenizer.index_word.get(token, '')\n",
    "        if word in ['<start>', '<end>', '<pad>']:\n",
    "            continue\n",
    "        output_text.append(word)\n",
    "\n",
    "    return ' '.join(output_text)\n",
    "\n",
    "def normalize_urdu_text(text):\n",
    "    \"\"\"Normalize Urdu text by removing extra spaces and standardizing characters\"\"\"\n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # Add any specific Urdu text normalization rules here if needed\n",
    "    return text\n",
    "\n",
    "def calculate_rouge_scores(reference, hypothesis):\n",
    "    \"\"\"Calculate ROUGE scores with character-level matching for Urdu\"\"\"\n",
    "    if not reference or not hypothesis:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    # Normalize texts\n",
    "    reference = normalize_urdu_text(reference)\n",
    "    hypothesis = normalize_urdu_text(hypothesis)\n",
    "\n",
    "    # Split into characters for character-level matching\n",
    "    ref_chars = list(reference)\n",
    "    hyp_chars = list(hypothesis)\n",
    "\n",
    "    # Calculate character-level overlap\n",
    "    ref_counter = Counter(ref_chars)\n",
    "    hyp_counter = Counter(hyp_chars)\n",
    "\n",
    "    # Common characters\n",
    "    common_chars = sum((ref_counter & hyp_counter).values())\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision = common_chars / len(hyp_chars) if hyp_chars else 0\n",
    "    recall = common_chars / len(ref_chars) if ref_chars else 0\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def create_test_pairs(english_data, urdu_data, test_size=0.01):\n",
    "    \"\"\"Create test pairs from the dataset\"\"\"\n",
    "    # Calculate number of test examples\n",
    "    num_test = int(len(english_data) * test_size)\n",
    "\n",
    "    # Create test pairs\n",
    "    test_pairs = list(zip(english_data[-num_test:], urdu_data[-num_test:]))\n",
    "\n",
    "    return test_pairs\n",
    "\n",
    "def evaluate_model(transformer, test_pairs, num_examples=100):\n",
    "    \"\"\"Evaluate the model using BLEU and custom ROUGE scores\"\"\"\n",
    "    # Initialize lists to store references and hypotheses\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    # Initialize smoothing function for BLEU\n",
    "    smoothie = SmoothingFunction().method1\n",
    "\n",
    "    print(\"\\nEvaluating model on test set...\")\n",
    "\n",
    "    # Limit evaluation to specified number of examples\n",
    "    test_pairs = test_pairs[:num_examples]\n",
    "\n",
    "    # Store all scores\n",
    "    all_rouge_scores = []\n",
    "\n",
    "    for i, (eng, urdu) in enumerate(test_pairs):\n",
    "        try:\n",
    "            # Translate English sentence\n",
    "            predicted_urdu = translate(transformer, eng)\n",
    "\n",
    "            # Tokenize reference and hypothesis for BLEU\n",
    "            reference_tokens = urdu.split()\n",
    "            hypothesis_tokens = predicted_urdu.split()\n",
    "\n",
    "            # Store for BLEU calculation\n",
    "            references.append([reference_tokens])\n",
    "            hypotheses.append(hypothesis_tokens)\n",
    "\n",
    "            # Calculate custom ROUGE scores\n",
    "            precision, recall, f1 = calculate_rouge_scores(urdu, predicted_urdu)\n",
    "            all_rouge_scores.append((precision, recall, f1))\n",
    "\n",
    "            if i < 5:  # Print first 5 examples\n",
    "                print(f\"\\nExample {i+1}:\")\n",
    "                print(f\"English: {eng}\")\n",
    "                print(f\"Reference Urdu: {urdu}\")\n",
    "                print(f\"Predicted Urdu: {predicted_urdu}\")\n",
    "                print(f\"Character-level Precision: {precision:.4f}\")\n",
    "                print(f\"Character-level Recall: {recall:.4f}\")\n",
    "                print(f\"Character-level F1: {f1:.4f}\")\n",
    "\n",
    "                # Print character-level analysis\n",
    "                print(\"\\nCharacter-level Analysis:\")\n",
    "                ref_chars = set(urdu)\n",
    "                hyp_chars = set(predicted_urdu)\n",
    "                common_chars = ref_chars.intersection(hyp_chars)\n",
    "                print(f\"Common characters: {len(common_chars)}\")\n",
    "                print(f\"Reference unique characters: {len(ref_chars)}\")\n",
    "                print(f\"Prediction unique characters: {len(hyp_chars)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {i}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Calculate corpus BLEU score\n",
    "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
    "\n",
    "    # Calculate average ROUGE scores\n",
    "    avg_precision = np.mean([score[0] for score in all_rouge_scores])\n",
    "    avg_recall = np.mean([score[1] for score in all_rouge_scores])\n",
    "    avg_f1 = np.mean([score[2] for score in all_rouge_scores])\n",
    "\n",
    "    print(\"\\nOverall Evaluation Metrics:\")\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print(f\"Average Character-level Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Character-level Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average Character-level F1: {avg_f1:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'bleu': bleu_score,\n",
    "        'char_precision': avg_precision,\n",
    "        'char_recall': avg_recall,\n",
    "        'char_f1': avg_f1\n",
    "    }\n",
    "\n",
    "# Add this helper function to analyze translation quality\n",
    "def analyze_translation_quality(reference, prediction):\n",
    "    \"\"\"Analyze the quality of translation by comparing character overlap\"\"\"\n",
    "    ref_chars = set(reference)\n",
    "    pred_chars = set(prediction)\n",
    "\n",
    "    common_chars = ref_chars.intersection(pred_chars)\n",
    "    missing_chars = ref_chars - pred_chars\n",
    "    extra_chars = pred_chars - ref_chars\n",
    "\n",
    "    return {\n",
    "        'common_chars': len(common_chars),\n",
    "        'missing_chars': len(missing_chars),\n",
    "        'extra_chars': len(extra_chars),\n",
    "        'reference_length': len(reference),\n",
    "        'prediction_length': len(prediction)\n",
    "    }\n",
    "\n",
    "# Use the evaluation function\n",
    "def evaluate_with_analysis(transformer, test_pairs, num_examples=100):\n",
    "    \"\"\"Run evaluation with detailed analysis\"\"\"\n",
    "    results = evaluate_model(transformer, test_pairs, num_examples)\n",
    "\n",
    "    # Add detailed analysis for a few examples\n",
    "    print(\"\\nDetailed Analysis of First 3 Examples:\")\n",
    "    for i, (eng, urdu) in enumerate(test_pairs[:3]):\n",
    "        predicted_urdu = translate(transformer, eng)\n",
    "        analysis = analyze_translation_quality(urdu, predicted_urdu)\n",
    "\n",
    "        print(f\"\\nExample {i+1} Analysis:\")\n",
    "        print(f\"Reference length: {analysis['reference_length']}\")\n",
    "        print(f\"Prediction length: {analysis['prediction_length']}\")\n",
    "        print(f\"Common characters: {analysis['common_chars']}\")\n",
    "        print(f\"Missing characters: {analysis['missing_chars']}\")\n",
    "        print(f\"Extra characters: {analysis['extra_chars']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Initialize a new transformer\n",
    "transformer = initialize_transformer()\n",
    "\n",
    "# Load the weights\n",
    "weights_path = \"english_to_urdu_transformer.keras\"  # Adjust path as needed\n",
    "transformer.load_weights(weights_path)\n",
    "\n",
    "# Create test pairs\n",
    "test_pairs = create_test_pairs(english_data, urdu_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = evaluate_model(transformer, test_pairs, num_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DkSMmPsxbcFn",
    "outputId": "9945ddc8-0f6a-4e84-8115-aede2ac49d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Sentence: سورج کی قَسم اور اس کی روشنی کی قَسم ۔\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(transformer, sentence, english_tokenizer, urdu_tokenizer, max_length):\n",
    "    # Preprocess the input sentence\n",
    "    sentence = '<start> ' + sentence.strip() + ' <end>'\n",
    "    input_sequence = english_tokenizer.texts_to_sequences([sentence])\n",
    "    input_sequence = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        input_sequence,\n",
    "        maxlen=max_length,\n",
    "        padding='post',\n",
    "        truncating='post'\n",
    "    )\n",
    "\n",
    "    # Create masks\n",
    "    enc_padding_mask = create_padding_mask(input_sequence)\n",
    "\n",
    "    # Initialize the decoder input with the start token\n",
    "    start_token = urdu_tokenizer.word_index['<start>']\n",
    "    end_token = urdu_tokenizer.word_index['<end>']\n",
    "    decoder_input = tf.expand_dims([start_token], 0)\n",
    "\n",
    "    # Initialize the translated sentence\n",
    "    translated_sentence = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(decoder_input)[1])\n",
    "        dec_padding_mask = create_padding_mask(decoder_input)\n",
    "        combined_mask = tf.maximum(dec_padding_mask, look_ahead_mask)\n",
    "\n",
    "        # Pass through the model\n",
    "        predictions = transformer(\n",
    "            inputs=(input_sequence, decoder_input, enc_padding_mask, combined_mask, enc_padding_mask),\n",
    "            training=False\n",
    "        )\n",
    "\n",
    "        # Get the predicted token\n",
    "        predicted_id = tf.argmax(predictions[:, -1:, :], axis=-1).numpy()[0][0]\n",
    "\n",
    "        # Break if end token is predicted\n",
    "        if predicted_id == end_token:\n",
    "            break\n",
    "\n",
    "        # Append the predicted token to the translated sentence\n",
    "        translated_sentence.append(predicted_id)\n",
    "\n",
    "        # Update the decoder input\n",
    "        decoder_input = tf.concat([decoder_input, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    # Convert the translated sentence from token ids to words\n",
    "    translated_sentence = urdu_tokenizer.sequences_to_texts([translated_sentence])[0]\n",
    "\n",
    "    return translated_sentence\n",
    "\n",
    "# Example usage\n",
    "custom_sentence = \"By the sun and by its brightness\"\n",
    "translated_sentence = translate_sentence(transformer, custom_sentence, english_tokenizer, urdu_tokenizer, max_sequence_length)\n",
    "print(f\"Translated Sentence: {translated_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
